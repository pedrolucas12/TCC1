# Metodologia

> Abordagem metodol√≥gica completa para desenvolvimento do modelo de previs√£o de surtos de dengue.

---

## üéØ Vis√£o Geral

Este documento descreve a metodologia completa do TCC, desde a coleta de dados at√© a avalia√ß√£o final dos modelos preditivos.

---

## üìä 1. PIPELINE GERAL

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    METODOLOGIA COMPLETA                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   1. COLETA DE DADOS (ETL)          ‚îÇ
        ‚îÇ   - SINAN (casos dengue)            ‚îÇ
        ‚îÇ   - CHIRPS/ERA5 (clima)             ‚îÇ
        ‚îÇ   - IBGE (popula√ß√£o, geometrias)    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   2. LIMPEZA E NORMALIZA√á√ÉO         ‚îÇ
        ‚îÇ   - Padroniza√ß√£o de datas           ‚îÇ
        ‚îÇ   - C√≥digos IBGE (7 d√≠gitos)        ‚îÇ
        ‚îÇ   - Filtro de casos prov√°veis       ‚îÇ
        ‚îÇ   - Tratamento de missing           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   3. AGREGA√á√ÉO ESPACIAL/TEMPORAL    ‚îÇ
        ‚îÇ   - Casos por munic√≠pio-semana      ‚îÇ
        ‚îÇ   - Zonal stats clima ‚Üí munic√≠pio   ‚îÇ
        ‚îÇ   - Join com popula√ß√£o              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   4. FEATURE ENGINEERING            ‚îÇ
        ‚îÇ   - Lags (1-12 semanas)             ‚îÇ
        ‚îÇ   - M√©dias m√≥veis (4, 8 semanas)    ‚îÇ
        ‚îÇ   - Anomalias clim√°ticas            ‚îÇ
        ‚îÇ   - Vari√°veis socioecon√¥micas       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   5. AN√ÅLISE EXPLORAT√ìRIA           ‚îÇ
        ‚îÇ   - Distribui√ß√µes, tend√™ncias       ‚îÇ
        ‚îÇ   - Correla√ß√µes cruzadas            ‚îÇ
        ‚îÇ   - Sazonalidade                    ‚îÇ
        ‚îÇ   - Visualiza√ß√µes                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   6. SELE√á√ÉO DE FEATURES            ‚îÇ
        ‚îÇ   - Correla√ß√£o cruzada (CCF)        ‚îÇ
        ‚îÇ   - Import√¢ncia de features (RF)    ‚îÇ
        ‚îÇ   - SHAP values                     ‚îÇ
        ‚îÇ   - Windowed correlation            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   7. DIVIS√ÉO TREINO/VALIDA√á√ÉO/TESTE ‚îÇ
        ‚îÇ   - TimeSeriesSplit (rolling)       ‚îÇ
        ‚îÇ   - Sem vazamento temporal          ‚îÇ
        ‚îÇ   - Valida√ß√£o cruzada temporal      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   8. MODELAGEM                      ‚îÇ
        ‚îÇ   Baseline: SARIMA, Prophet         ‚îÇ
        ‚îÇ   ML: RF, XGBoost                   ‚îÇ
        ‚îÇ   DL: LSTM, BiLSTM                  ‚îÇ
        ‚îÇ   Ensemble: Voting/Stacking         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   9. AVALIA√á√ÉO E COMPARA√á√ÉO         ‚îÇ
        ‚îÇ   - RMSE, MAE, MAPE                 ‚îÇ
        ‚îÇ   - AUC, Precision, Recall (surtos) ‚îÇ
        ‚îÇ   - An√°lise por munic√≠pio/regi√£o    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   10. INTERPRETA√á√ÉO                 ‚îÇ
        ‚îÇ   - SHAP para XGBoost               ‚îÇ
        ‚îÇ   - Feature importance              ‚îÇ
        ‚îÇ   - An√°lise de lags √≥timos          ‚îÇ
        ‚îÇ   - Casos de sucesso/falha          ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   11. SISTEMA DE ALERTA             ‚îÇ
        ‚îÇ   - Calibra√ß√£o de limiares          ‚îÇ
        ‚îÇ   - Defini√ß√£o de "surto"            ‚îÇ
        ‚îÇ   - Dashboard operacional           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß 2. DETALHAMENTO POR ETAPA

### ETAPA 1: Coleta de Dados (ETL)

#### Dados Epidemiol√≥gicos
- **Fonte**: OpenDataSUS / SINAN
- **Per√≠odo**: 2010-2024 (ou mais recente)
- **M√©todo**: Download via S3 (JSON por ano)
- **Armazenamento**: Raw layer (arquivos originais + metadata)

#### Dados Clim√°ticos
- **Precipita√ß√£o**: CHIRPS (NetCDF, di√°rio)
- **Temperatura/Umidade**: ERA5 ou NASA POWER
- **Per√≠odo**: Sincronizar com dados epidemiol√≥gicos
- **Armazenamento**: NetCDF ou Parquet particionado

#### Dados Auxiliares
- **Malhas**: Shapefile IBGE (munic√≠pios)
- **Popula√ß√£o**: Estimativas anuais IBGE
- **Armazenamento**: GeoPackage ou PostGIS

---

### ETAPA 2: Limpeza e Normaliza√ß√£o

#### Tratamentos Necess√°rios

**Datas**
```python
# Converter datas para datetime
df['dt_sintomas'] = pd.to_datetime(df['dt_sin_pri'], errors='coerce')
df['dt_notific'] = pd.to_datetime(df['dt_notific'], errors='coerce')

# Extrair semana epidemiol√≥gica
df['ano_semana'] = df['sem_pri']  # formato YYYYWW
df['ano'] = df['ano_semana'].str[:4].astype(int)
df['semana'] = df['ano_semana'].str[4:].astype(int)
```

**C√≥digos Municipais**
```python
# Padronizar c√≥digo IBGE (7 d√≠gitos)
df['ibge_municipio'] = df['id_municip'].astype(str).str.zfill(7)
```

**Filtro de Casos**
```python
# Manter apenas casos prov√°veis/confirmados
filtro_casos = df['criterio_confirmacao'].isin([
    'LABORATORIAL',
    'CL√çNICO-EPIDEMIOL√ìGICO'
])
df_filtrado = df[filtro_casos]
```

**Tratamento de Missing**
- Identificar % de missing por coluna
- Imputar ou remover conforme criticidade
- Documentar decis√µes

---

### ETAPA 3: Agrega√ß√£o Espacial e Temporal

#### Agrega√ß√£o Temporal de Casos

```python
# Agregar casos por munic√≠pio-semana
casos_semana = df_filtrado.groupby([
    'ibge_municipio', 
    'ano', 
    'semana'
]).agg({
    'id_notificacao': 'count',  # Total de casos
    'idade': 'mean',            # Idade m√©dia
    'evolucao': lambda x: (x == '√ìBITO').sum()  # √ìbitos
}).reset_index()

casos_semana.columns = ['ibge_municipio', 'ano', 'semana', 
                        'casos', 'idade_media', 'obitos']
```

#### Agrega√ß√£o Espacial de Clima

**Zonal Statistics (CHIRPS ‚Üí Munic√≠pio)**

```python
import geopandas as gpd
import rasterio
from rasterstats import zonal_stats

# Carregar shapefile munic√≠pios
municipios = gpd.read_file('malha_municipal.shp')

# Para cada dia de dados CHIRPS
stats = zonal_stats(
    municipios.geometry,
    'chirps_2024_01_01.tif',
    stats=['mean', 'sum', 'min', 'max']
)

# Resultado: precipita√ß√£o m√©dia por munic√≠pio
municipios['precip_mm'] = [s['mean'] for s in stats]
```

#### Agrega√ß√£o Temporal de Clima (Di√°rio ‚Üí Semanal)

```python
# Converter para semanal (soma para chuva, m√©dia para temperatura)
clima_semanal = clima_diario.groupby([
    'ibge_municipio',
    pd.Grouper(key='data', freq='W-MON')  # Semana come√ßa segunda
]).agg({
    'precipitacao': 'sum',      # Soma da semana
    'temperatura': 'mean',       # M√©dia da semana
    'umidade': 'mean'
}).reset_index()
```

---

### ETAPA 4: Feature Engineering

#### Features Temporais (Lags)

```python
def create_lags(df, col, lags=12):
    """Criar features de lag para uma coluna"""
    for lag in range(1, lags + 1):
        df[f'{col}_lag{lag}'] = df.groupby('ibge_municipio')[col].shift(lag)
    return df

# Aplicar para vari√°veis clim√°ticas
df = create_lags(df, 'precipitacao', lags=12)
df = create_lags(df, 'temperatura', lags=12)
df = create_lags(df, 'umidade', lags=8)

# Aplicar para casos passados (autoregressivo)
df = create_lags(df, 'casos', lags=4)
```

#### M√©dias M√≥veis

```python
# M√©dia m√≥vel de 4 semanas
df['precip_ma4'] = df.groupby('ibge_municipio')['precipitacao'].transform(
    lambda x: x.rolling(window=4, min_periods=1).mean()
)

# M√©dia m√≥vel de 8 semanas
df['temp_ma8'] = df.groupby('ibge_municipio')['temperatura'].transform(
    lambda x: x.rolling(window=8, min_periods=1).mean()
)
```

#### Somas Acumuladas

```python
# Precipita√ß√£o acumulada √∫ltimas 4 semanas
df['precip_cum4'] = df.groupby('ibge_municipio')['precipitacao'].transform(
    lambda x: x.rolling(window=4, min_periods=1).sum()
)
```

#### Anomalias Clim√°ticas

```python
# Calcular climatologia (m√©dia hist√≥rica por semana do ano)
climatologia = df.groupby(['ibge_municipio', 'semana'])['temperatura'].mean()

# Anomalia = observado - climatologia
df['temp_anomalia'] = df.apply(
    lambda row: row['temperatura'] - climatologia.loc[
        (row['ibge_municipio'], row['semana'])
    ],
    axis=1
)
```

#### Features Socioecon√¥micas

```python
# Join com dados IBGE
df = df.merge(populacao_ibge, on='ibge_municipio', how='left')

# Taxa per capita
df['casos_per_100k'] = (df['casos'] / df['populacao']) * 100000

# Log de popula√ß√£o (escala)
df['log_populacao'] = np.log1p(df['populacao'])
```

#### Features Espaciais

```python
# Casos em munic√≠pios vizinhos (windowed correlation)
# Requer matriz de adjac√™ncia

# Latitude/Longitude do centroid
df['lat'] = df['ibge_municipio'].map(municipios.set_index('codigo')['lat_centroid'])
df['lon'] = df['ibge_municipio'].map(municipios.set_index('codigo')['lon_centroid'])
```

#### Features Temporais (Sazonalidade)

```python
# Semana do ano (c√≠clica)
df['semana_sin'] = np.sin(2 * np.pi * df['semana'] / 52)
df['semana_cos'] = np.cos(2 * np.pi * df['semana'] / 52)

# M√™s
df['mes'] = pd.to_datetime(
    df['ano'].astype(str) + df['semana'].astype(str) + '1',
    format='%Y%W%w'
).dt.month
```

---

### ETAPA 5: An√°lise Explorat√≥ria de Dados (EDA)

#### Objetivos
1. Entender distribui√ß√µes de casos e clima
2. Identificar tend√™ncias e sazonalidade
3. Detectar correla√ß√µes
4. Identificar outliers/anomalias

#### An√°lises Principais

**Distribui√ß√£o de Casos**
- Histograma de casos por semana
- Casos por munic√≠pio (mapa)
- Evolu√ß√£o temporal (s√©rie temporal)

**Sazonalidade**
- Boxplot de casos por m√™s
- Decomposi√ß√£o de s√©rie temporal (tend√™ncia, sazonalidade, res√≠duo)

**Correla√ß√µes**
- Matriz de correla√ß√£o entre vari√°veis clim√°ticas e casos
- Cross-correlation function (CCF) para identificar lags √≥timos

**An√°lise Espacial**
- Mapa coropl√©tico de incid√™ncia
- Clusters espaciais (Moran's I)

---

### ETAPA 6: Sele√ß√£o de Features

#### M√©todos

**1. Correla√ß√£o Cruzada (Cross-Correlation Function)**
```python
from statsmodels.tsa.stattools import ccf

# CCF entre temperatura e casos (com lags)
correlacoes = ccf(df['temperatura'], df['casos'], adjusted=False)

# Identificar lag com maior correla√ß√£o
lag_otimo = np.argmax(np.abs(correlacoes))
```

**2. Feature Importance (Random Forest)**
```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)

# Import√¢ncias
importancias = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
```

**3. SHAP (Interpretabilidade)**
```python
import shap

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Plot de import√¢ncia
shap.summary_plot(shap_values, X_test)
```

**4. Windowed Correlation** (Ferdousi et al., 2021)
- T√©cnica para usar dados de munic√≠pios vizinhos
- √ötil para munic√≠pios com poucos dados hist√≥ricos

---

### ETAPA 7: Divis√£o Treino/Valida√ß√£o/Teste

#### Princ√≠pio Fundamental
‚ö†Ô∏è **NUNCA vazar informa√ß√£o do futuro para o passado**

#### TimeSeriesSplit (Rolling Window)

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    
    # Treinar modelo
    model.fit(X_train, y_train)
    
    # Avaliar
    pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, pred, squared=False)
    print(f"Fold {fold}: RMSE = {rmse:.2f}")
```

#### Divis√£o Recomendada
- **Treino**: 70% (primeiros anos)
- **Valida√ß√£o**: 15% (anos intermedi√°rios) - para tuning
- **Teste**: 15% (√∫ltimos anos) - avalia√ß√£o final

Exemplo:
- Treino: 2010-2019
- Valida√ß√£o: 2020-2021
- Teste: 2022-2024

---

### ETAPA 8: Modelagem

#### Modelos a Serem Testados

#### 1. Baseline Estat√≠stico: SARIMA/SARIMAX

**Caracter√≠sticas**
- Modelo autoregressivo com sazonalidade
- SARIMAX permite vari√°veis ex√≥genas (clima)
- Interpret√°vel

**Implementa√ß√£o**
```python
from statsmodels.tsa.statespace.sarimax import SARIMAX

model = SARIMAX(
    y_train,
    order=(1, 1, 1),          # (p, d, q)
    seasonal_order=(1, 1, 1, 52),  # (P, D, Q, s) - sazonalidade semanal
    exog=X_train[['temperatura', 'precipitacao']]
)

results = model.fit()
forecast = results.forecast(steps=len(X_test), exog=X_test)
```

#### 2. Prophet (Facebook)

**Caracter√≠sticas**
- Lida bem com sazonalidade m√∫ltipla
- Robusto a missing data
- F√°cil de usar

**Implementa√ß√£o**
```python
from prophet import Prophet

model = Prophet(yearly_seasonality=True, weekly_seasonality=False)

# Adicionar regressores
model.add_regressor('temperatura')
model.add_regressor('precipitacao')

model.fit(train_df)
forecast = model.predict(test_df)
```

#### 3. Random Forest

**Caracter√≠sticas**
- Captura n√£o-linearidades
- Robusto a outliers
- Feature importance interpret√°vel

```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=200,
    max_depth=20,
    min_samples_split=10,
    random_state=42
)

rf.fit(X_train, y_train)
pred = rf.predict(X_test)
```

#### 4. XGBoost

**Caracter√≠sticas**
- Estado-da-arte para dados tabulares
- Muito usado em competi√ß√µes
- SHAP para interpreta√ß√£o

```python
import xgboost as xgb

xgb_model = xgb.XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='reg:squarederror',
    random_state=42
)

xgb_model.fit(X_train, y_train)
pred = xgb_model.predict(X_test)
```

#### 5. LSTM (Long Short-Term Memory)

**Caracter√≠sticas**
- Redes neurais recorrentes
- Captura depend√™ncias de longo prazo
- Estado-da-arte para s√©ries temporais

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# Reshapear dados para LSTM: (samples, timesteps, features)
X_train_lstm = X_train.values.reshape((X_train.shape[0], timesteps, n_features))

model = models.Sequential([
    layers.LSTM(64, activation='relu', input_shape=(timesteps, n_features)),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_split=0.2)

pred = model.predict(X_test_lstm)
```

#### 6. Ensemble (Stacking/Voting)

```python
from sklearn.ensemble import VotingRegressor

ensemble = VotingRegressor([
    ('rf', rf),
    ('xgb', xgb_model)
])

ensemble.fit(X_train, y_train)
pred = ensemble.predict(X_test)
```

---

### ETAPA 9: Avalia√ß√£o e Compara√ß√£o

#### M√©tricas para Previs√£o de Contagem

**RMSE (Root Mean Squared Error)**
```python
from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(y_true, y_pred, squared=False)
```

**MAE (Mean Absolute Error)**
```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_true, y_pred)
```

**MAPE (Mean Absolute Percentage Error)**
```python
mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1))) * 100
```

#### M√©tricas para Classifica√ß√£o de Surtos

**Definir Limiar de Surto**
```python
# Exemplo: percentil 75 hist√≥rico por munic√≠pio
limiares = df.groupby('ibge_municipio')['casos'].quantile(0.75)

df['surto'] = df.apply(
    lambda row: 1 if row['casos'] > limiares[row['ibge_municipio']] else 0,
    axis=1
)
```

**M√©tricas de Classifica√ß√£o**
```python
from sklearn.metrics import roc_auc_score, precision_score, recall_score

auc = roc_auc_score(y_true_class, y_pred_proba)
precision = precision_score(y_true_class, y_pred_class)
recall = recall_score(y_true_class, y_pred_class)
```

#### Compara√ß√£o de Modelos

```python
resultados = pd.DataFrame({
    'Modelo': ['SARIMA', 'Prophet', 'RF', 'XGBoost', 'LSTM'],
    'RMSE': [rmse_sarima, rmse_prophet, rmse_rf, rmse_xgb, rmse_lstm],
    'MAE': [mae_sarima, mae_prophet, mae_rf, mae_xgb, mae_lstm],
    'AUC': [auc_sarima, auc_prophet, auc_rf, auc_xgb, auc_lstm]
})

print(resultados.sort_values('RMSE'))
```

---

### ETAPA 10: Interpreta√ß√£o

#### SHAP para XGBoost
```python
import shap

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Plot
shap.summary_plot(shap_values, X_test)
shap.dependence_plot('temperatura_lag4', shap_values, X_test)
```

#### An√°lise de Lags √ìtimos
- Identificar quais lags s√£o mais importantes por regi√£o
- Comparar com literatura

---

### ETAPA 11: Sistema de Alerta

#### Calibra√ß√£o de Limiares
```python
# Limiar adaptativo por munic√≠pio
limiares = df.groupby('ibge_municipio')['casos'].apply(
    lambda x: x.quantile(0.75)  # Ajustar conforme necess√°rio
)

# Classificar previs√µes
df['alerta'] = df['pred_casos'] > df['ibge_municipio'].map(limiares)
```

---

## üìà 3. T√âCNICAS AVAN√áADAS

### Transfer Learning entre Munic√≠pios
- Treinar em munic√≠pios com muitos dados
- Aplicar em munic√≠pios com poucos dados
- Fine-tuning com dados locais

### Modelos Hier√°rquicos
- Modelos por regi√£o (Norte, Nordeste, etc.)
- Compartilhamento de par√¢metros

### Nowcasting
- Corre√ß√£o de atraso de notifica√ß√£o
- Previs√£o do presente (dados ainda n√£o consolidados)

---

## ‚úÖ 4. CHECKLIST METODOL√ìGICO

- [ ] Dados brutos coletados e armazenados
- [ ] ETL documentado e reproduz√≠vel
- [ ] Limpeza e qualidade verificadas
- [ ] Features criadas e documentadas
- [ ] EDA completa com visualiza√ß√µes
- [ ] Divis√£o temporal sem vazamento
- [ ] Modelos baseline implementados
- [ ] Modelos avan√ßados implementados
- [ ] Valida√ß√£o cruzada temporal realizada
- [ ] M√©tricas comparadas entre modelos
- [ ] Interpreta√ß√£o (SHAP) realizada
- [ ] Sistema de alerta calibrado
- [ ] C√≥digo versionado no Git
- [ ] Documenta√ß√£o completa

---

*√öltima atualiza√ß√£o: Outubro 2025*

